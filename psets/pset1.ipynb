{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b601bd-43e8-4d3e-b733-c8a3e6d58fe1",
   "metadata": {},
   "source": [
    "# 18.S097/16.S092 Problem Set 1\n",
    "\n",
    "Due Friday 4/11 at **11:59pm**; 20% penalty if it is turned in within 24 hours, and after that late psets will not be accepted without prior arrangement.   Submit in PDF format: a decent-quality scan/image of any handwritten solutions (e.g. get a scanner app on your phone or use a tablet), combined with a PDF printout of your Jupyter notebook showing your code and (clearly labeled) results.\n",
    "\n",
    "**TO GENERATE A PDF OF YOUR JUPYTER NOTEBOOK:** In the Jupyter client (e.g. the [JupyterLab Desktop](https://github.com/jupyterlab/jupyterlab-desktop) app), in the *File* pull-down menu, select *Save and Export Notebook As*, and then select the *HTML* format (not PDF, which may require special software).  Then open the downloaded HTML file with your favorite browser, and use the browser's *Print* function to generate the PDF file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1dbef1",
   "metadata": {},
   "source": [
    "## Problem 1 (5+5+5+5 points)\n",
    "\n",
    "Suppose that you are solving $Ax = b$ where $A$ is an $m \\times m$ (real) matrix of the form\n",
    "$$\n",
    "A = S + uv^T\n",
    "$$\n",
    "where $S$ is a *sparse* matrix (with $K$ nonzero entries) and $u,v$ are two $m$-component vectors.  That is, $A$ is the sum of a sparse matrix and a **rank-1** matrix.   We want to solve this efficiently for large $m$, even though $A$ itself is not sparse!\n",
    "\n",
    "**(a)** For example, give possible vectors $u,v$ if \n",
    "$$\n",
    "uv^T = \\alpha \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\end{pmatrix}\n",
    "$$\n",
    "or\n",
    "$$\n",
    "uv^T = \\alpha \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 1 & 1 & 1 & 1 \\end{pmatrix}\n",
    "$$\n",
    "for some scalar $\\alpha$.\n",
    "\n",
    "**(b)** Explain why you can efficiently multiply $A$ by any vector $x$, in cost *much* better than $O(m^2)$.  Give the complexity of your algorithm $O(\\cdots)$ in terms of $K$ and $m$.\n",
    "\n",
    "**(c)** Show by explicit multiplication $AA^{-1} = I$ that $A^{-1} = S^{-1} - \\frac{S^{-1} u v^T S^{-1}}{1 + v^T S^{-1} u}$.\n",
    "\n",
    "**(d)** Assume that $S$ has an efficient (low-fill-in) sparse LU factorization $PSP^T = LU$, with $P$ being a permutation of the rows and columns ($P^{-1} = P^T$, like any permutation).   Explain how you can exploit this to efficiently solve $Ax = b$ (using the formula from part **c**).\n",
    "\n",
    "*Note:* Don't try to compute $S^{-1}$ explicitly — it is probably not sparse!  Note that solving a triangular system like $Ly = c$ or $Uy = c$ is very efficient given sparse $L$ and $U$ vectors.  Also, permuting a vector, like $Py$ or $P^Ty$, can be done in $O(m)$ operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ac28da",
   "metadata": {},
   "source": [
    "## Problem 2 (10+10 points)\n",
    "\n",
    "[Niu et al. (2014)](https://aapm.onlinelibrary.wiley.com/doi/10.1118/1.4866386) describe an optimization problem (equation 12) to solve for an unknown vector $x \\in \\mathbb{R}^{2N}$ whose entries describe the pixels of a medical image to be reconstructed from measurements.\n",
    "\n",
    "You *don't* need to understand any of the physics, statistics, or interpretation of this paper.  You just need to be able to decipher the *math* of sections 2.A–2.C and **translate it into linear algebra**.  This is an important skill!\n",
    "\n",
    "**(a)** Show that equation 12 of this paper is equivalent to a quadratic optimization problem\n",
    "$$\n",
    "\\min_{x} \\left[ x^T M x - b^T x + \\alpha \\right]\n",
    "$$\n",
    "for some symmetric matrix $M = M^T$ and vector $b$ and a scalar $\\alpha$  (all independent of $x$), each of which can be computed efficiently.  (In fact, it is a *convex* quadratic problem because all of the terms are strictly nonnegative, but you need not show that.)\n",
    "\n",
    "Hence, by taking the gradient $\\nabla_x (x^T M x - b^T x + \\alpha)$ and setting it to zero, we obtain a simple linear system $Mx = b/2$ that we could solve for $x$.\n",
    "\n",
    "**(b)** Explain why your matrix $M$ is very *sparse* matrix (only a small number of nonzero entries per row).\n",
    "\n",
    "Hence you could solve for $x$ efficiently assuming we have a good way to solve sparse linear systems (e.g. a sparse-direct solver, though in the paper they eventually use a conjugate-gradient algorithm instead)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73089a",
   "metadata": {},
   "source": [
    "## Problem 3 (5 + 5 + 10 points)\n",
    "\n",
    "Suppose that $A = A^T $ is the symmetric $m \\times m$ \"banded\" matrix:\n",
    "$$\n",
    "A = \\begin{pmatrix} a_1 & b_1 & c_1 & & & & & & &\\\\\n",
    "b_1 & a_2 & b_2 & c_2 & & & & &\\\\\n",
    "c_1 & b_2 & a_3 & b_3 & c_3 & & & & & \\\\\n",
    "& c_2 & b_3 & a_4 & b_4 & c_4 & & & & \\\\\n",
    "& & c_3 & b_4 & a_5 & b_5 & c_5 & & & \\\\\n",
    "& & & \\ddots & \\ddots & \\ddots & \\ddots & \\ddots & & \\\\\n",
    "& & &  & c_{m-4} & b_{m-3} & a_{m-2} & b_{m-2} & c_{m-2} \\\\\n",
    "& & & &  & c_{m-3} & b_{m-2} & a_{m-1} & b_{m-1} \\\\\n",
    "& & & & &  & c_{m-2} & b_{m-1} & a_m\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "which is defined in terms of three vectors $a \\in \\mathbb{R}^m$, $c \\in \\mathbb{R}^{m-1}$, and $c \\in \\mathbb{R}^{m-2}$.\n",
    "\n",
    "**(a)** Sketch the sparsity pattern of this matrix as a graph, as defined in class ($m$ nodes representing rows and columns, with edges representing nonzero entries).   You can assume all of the elements of $a,b,c$ are nonzero.\n",
    "\n",
    "**(b)** Write code (in Julia using the `SparseArrays` package, or in Python with `scipy.sparse`) that creates an (unstructured) sparse matrix $A$ given vectors $a,b,c$.   (The function `spdiagm` in Julia or `diags_array` in Python might be helpful.)  Plot the sparsity pattern (e.g. with `pyplot.spy`) for $m = 20$ to make sure it looks right.\n",
    "\n",
    "**(c)** Benchmark the performance (for a set of logarithmically spaced $m$ values up to $m = 10^7$) of solving $A x = y$ (e.g with `x = A \\ y` in Julia or `spsolve` in Python) for random $a,b,c,y$ and plot it as a function of $m$ on a log–log scale.  What power-law do you observe?  Does it make sense given the material from class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff0c62d",
   "metadata": {},
   "source": [
    "## Problem 4 (10 + 10 points)\n",
    "\n",
    "In class (lecture 2) we defined the compressed-sparse-column (CSC) format, in which a sparse matrix is defined by three arrays (denoted in Matlab notation as `Pr, Ir, Jc` in the slides) of nonzero values by column (`Pr`), row indices (`Ir`), and the starting (1-based) index of each column (`Jc`).\n",
    "\n",
    "As an example, `Pr = [31,41,59,26,53]`, `Ir = [1,3,2,3,1]`, and `Jc = [1,3,5,6]` describe the $3 \\times 3$ \"sparse\" matrix:\n",
    "$$\n",
    "A = \\begin{pmatrix} 31 & \\cdot & 53 \\\\ \\cdot & 59 & \\cdot \\\\ 41 & 26 & \\cdot \\end{pmatrix}\n",
    "$$\n",
    "(where $\\cdot$ indicates zero values that are not stored).\n",
    "\n",
    "**(a)** Write a function `matvec(Pr, Ir, Jc, x)`, in the language of your choice, that returns the matrix-vector product `A*x` given the `Pr,Ir,Jc` description of a sparse matrix $A$.  The cost of your function should be proportional to the number of nonzero entries, and you should allocate no arrays other than the output vector.   (You may *not* use a pre-existing sparse-matrix library — write `matvec` from scratch.)   Test it by multipling the $3 \\times 3$ matrix $A$ above by the 3 columns of $I$ and show that you obtain the columns of the same matrix as shown.\n",
    "\n",
    "**(b)** Write a function `matvecT(Pr, Ir, Jc, x)` similar to `matvec`, above, but which returns $A^T x$.  Again, the cost should be proportional to the number of nonzero entries, and you should allocate no arrays other than the output vector.  (This is equivalent to multiplying by $A^T$ stored in compressed sparse *row* format.)  Again, test it with the same $3 \\times 3$ matrix $A$ by the 3 columns of $I$ and show that you obtain the columns of $A^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e1cfe0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
